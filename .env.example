# PsyFind Environment Configuration

# LLM Provider Selection
# Choose which LLM service to use: ollama, openai, openrouter, fallback, auto
# - ollama: Use local Ollama only
# - openai: Use OpenAI GPT-4o only  
# - openrouter: Use OpenRouter Claude 3 only
# - fallback: Use basic structured reports only
# - auto: Try services in priority order (default)
LLM_PROVIDER=auto

# Ollama Configuration (Local LLM)
# Install Ollama from https://ollama.ai and run: ollama pull llama3.2
OLLAMA_URL=http://localhost:11434

# OpenAI Configuration (Cloud LLM)
# Get API key from https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# OpenRouter Configuration (Cloud LLM)
# Get API key from https://openrouter.ai
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Flask Configuration
FLASK_ENV=development
FLASK_DEBUG=True
FLASK_SECRET_KEY=your_secret_key_here

# Admin Panel Configuration
ADMIN_PASSWORD=admin123
DOCTOR_PASSWORD=doctor123
PASSWORD_SALT=psyfind_salt_2024

# Examples:
# LLM_PROVIDER=ollama          # Force use Ollama only
# LLM_PROVIDER=openai          # Force use OpenAI only
# LLM_PROVIDER=openrouter      # Force use OpenRouter only
# LLM_PROVIDER=fallback        # No LLM, basic reports only
# LLM_PROVIDER=auto            # Auto-select (Ollama → OpenAI → OpenRouter → Fallback)
